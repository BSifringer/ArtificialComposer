{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning on Reber-Grammar\n",
    "\n",
    "Warm up on the Reber-Grammar prediction problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, TimeDistributed, Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import reber_utility as ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining important parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_size = 50\n",
    "n_step=100 #redefine this. IT is to make 1000 reber words\n",
    "\n",
    "batch_size = 16\n",
    "n_batch = 12 # total number\n",
    "n_test_batch = 2 # testing part\n",
    "\n",
    "train_idxes = np.array(range(0,(n_batch-n_test_batch)*batch_size))\n",
    "test_idxes = np.array(range((n_batch-n_test_batch)*batch_size, n_batch*batch_size))\n",
    "\n",
    "N_Epoch = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate and vectorize Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ru.generate_reber_machine_discrete()\n",
    "\n",
    "X,Y = m.to_X_and_Y(m.make_words(n_batch*batch_size,min_steps=(step_size*n_step+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 5000, 6)\n",
      "(192, 5000, 6)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "For the masking layer the value \"-1\" was chosen since to categorical already uses the values \"0\" and \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Masking(mask_value= -1., batch_input_shape=(batch_size,step_size,len(m.transitions))))\n",
    "model.add(LSTM(20, return_sequences=True, batch_input_shape=(batch_size,step_size,len(m.transitions)),stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(len(m.transitions))))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)#learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "train model and generate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to learn:\n",
      "------- 1 out of 5 Epoch -----\n",
      "batch 1 of 10\n",
      "batch 2 of 10\n",
      "batch 3 of 10\n",
      "batch 4 of 10\n",
      "batch 5 of 10\n",
      "batch 6 of 10\n",
      "batch 7 of 10\n",
      "batch 8 of 10\n",
      "batch 9 of 10\n",
      "batch 10 of 10\n",
      "Train results:\t ['loss', 'acc'] \n",
      " \t [ 49.61677154  79.53800003]\n",
      "Test results:\t ['loss', 'acc'] \n",
      " \t [ 48.85262887  79.43749997]\n",
      "------- 2 out of 5 Epoch -----\n",
      "batch 1 of 10\n",
      "batch 2 of 10\n",
      "batch 3 of 10\n",
      "batch 4 of 10\n",
      "batch 5 of 10\n",
      "batch 6 of 10\n",
      "batch 7 of 10\n",
      "batch 8 of 10\n",
      "batch 9 of 10\n",
      "batch 10 of 10\n",
      "Train results:\t ['loss', 'acc'] \n",
      " \t [ 48.91956176  79.56162502]\n",
      "Test results:\t ['loss', 'acc'] \n",
      " \t [ 49.03371318  79.45499977]\n",
      "------- 3 out of 5 Epoch -----\n",
      "batch 1 of 10\n",
      "batch 2 of 10\n",
      "batch 3 of 10\n",
      "batch 4 of 10\n",
      "batch 5 of 10\n",
      "batch 6 of 10\n",
      "batch 7 of 10\n",
      "batch 8 of 10\n",
      "batch 9 of 10\n",
      "batch 10 of 10\n",
      "Train results:\t ['loss', 'acc'] \n",
      " \t [ 48.8486317   79.54062497]\n",
      "Test results:\t ['loss', 'acc'] \n",
      " \t [ 48.82096468  79.4562498 ]\n",
      "------- 4 out of 5 Epoch -----\n",
      "batch 1 of 10\n",
      "batch 2 of 10\n",
      "batch 3 of 10\n",
      "batch 4 of 10\n",
      "batch 5 of 10\n",
      "batch 6 of 10\n",
      "batch 7 of 10\n",
      "batch 8 of 10\n",
      "batch 9 of 10\n",
      "batch 10 of 10\n",
      "Train results:\t ['loss', 'acc'] \n",
      " \t [ 48.82746233  79.53787495]\n",
      "Test results:\t ['loss', 'acc'] \n",
      " \t [ 48.90266281  79.43812498]\n",
      "------- 5 out of 5 Epoch -----\n",
      "batch 1 of 10\n",
      "batch 2 of 10\n",
      "batch 3 of 10\n",
      "batch 4 of 10\n",
      "batch 5 of 10\n",
      "batch 6 of 10\n",
      "batch 7 of 10\n",
      "batch 8 of 10\n",
      "batch 9 of 10\n",
      "batch 10 of 10\n",
      "Train results:\t ['loss', 'acc'] \n",
      " \t [ 48.8058327   79.55199998]\n",
      "Test results:\t ['loss', 'acc'] \n",
      " \t [ 48.90486845  79.45687482]\n"
     ]
    }
   ],
   "source": [
    "metrics_train = np.zeros((N_Epoch,len(model.metrics_names)))\n",
    "metrics_test = np.zeros((N_Epoch,len(model.metrics_names)))\n",
    "\n",
    "print('Starting to learn:')\n",
    "for i in range(N_Epoch):\n",
    "    print('------- {} out of {} Epoch -----'.format(i+1,N_Epoch))\n",
    "\n",
    "    ## Epochs should take all data; batches presented random, reset at each end of batch_size\n",
    "    np.random.shuffle(train_idxes)\n",
    "    batches_idxes = np.reshape(train_idxes, (-1,batch_size))\n",
    "    for j, batch  in enumerate(batches_idxes):\n",
    "        print('batch {} of {}'.format(j+1,n_batch-n_test_batch))\n",
    "        for k in range(n_step):\n",
    "            metrics_train[i] += model.train_on_batch(X[batch,k*step_size:(k+1)*(step_size)], Y[batch,k*step_size:(k+1)*step_size]) #python 0:3 gives 0,1,2 (which is not intuitive at all)\n",
    "        model.reset_states() \n",
    "\n",
    "    test_batch_idxes = np.reshape(test_idxes,(-1,batch_size))\n",
    "    for test_batch in test_batch_idxes:\n",
    "        for k in range(n_step):\n",
    "            metrics_test[i] += model.test_on_batch(X[test_batch,k*step_size:(k+1)*(step_size)], Y[test_batch,k*step_size:(k+1)*step_size])\n",
    "        model.reset_states() \n",
    "\n",
    "    print('Train results:\\t {} \\n \\t {}'.format(model.metrics_names, metrics_train[i]/(n_batch-n_test_batch) ))\n",
    "    print('Test results:\\t {} \\n \\t {}'.format(model.metrics_names, metrics_test[i]/(n_test_batch) ))\n",
    "    \n",
    "    #TODO: plot graph of accuracy, loss with errorbars dynamicly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't forget to save everything ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('embedCerg_model_2.h5')\n",
    "np.save('embedXdata_2.npy',X)\n",
    "np.save('embedydata_2.npy',Y)\n",
    "np.save('embedTrainMetrics_2', metrics_train)\n",
    "np.save('embedTestMetrics_2', metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
