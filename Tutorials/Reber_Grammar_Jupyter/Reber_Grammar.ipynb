{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning on Reber-Grammar\n",
    "\n",
    "Warm up on the Reber-Grammar prediction problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, TimeDistributed, Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "import reber_utility as r_u\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some examples of the reber_utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate continuous reber-words\n",
    "Data = r_u.generate_continuous_reberwords(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Data to indices\n",
    "indices = [[r_u.char2index[d] for d in Data[i]] for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert indices back to characters\n",
    "chars = [[r_u.index2char[d] for d in indices[i]] for i in range(len(indices))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining important parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_size = 50\n",
    "n_step=100 #redefine this. IT is to make 1000 reber words\n",
    "\n",
    "batch_size = 16\n",
    "n_batch = 12 # total number\n",
    "n_test_batch = 2 # testing part\n",
    "\n",
    "train_idxes = np.array(range(0,(n_batch-n_test_batch)*batch_size))\n",
    "test_idxes = np.array(range((n_batch-n_test_batch)*batch_size, n_batch*batch_size))\n",
    "\n",
    "N_Epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate and vectorize Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = r_u.generate_continuous_reberwords(n_batch*batch_size,step_size*n_step+1 - 20)\n",
    "\n",
    "#vectorize (should be also put into utility ;)\n",
    "\n",
    "X = -1*np.ones((n_batch*batch_size, step_size*n_step+1, len(r_u.char2index)), dtype=np.float64)\n",
    "for j, word in enumerate(D):\n",
    "    categorized = np_utils.to_categorical([r_u.char2index[c] for c in word])\n",
    "    X[j,:min(len(word,),step_size*n_step+1)] =  categorized[:min(len(word),step_size*n_step+1)]\n",
    "\n",
    "Y = np.roll(X,-1,axis=1)\n",
    "X = np.delete(X,-1,axis=1)\n",
    "Y = np.delete(Y,-1,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "For the masking layer the value \"-1\" was chosen since to categorical already uses the values \"0\" and \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Masking(mask_value= -1., batch_input_shape=(batch_size,step_size,len(chars))))\n",
    "model.add(LSTM(128, return_sequences=True, batch_input_shape=(batch_size,step_size,len(chars)),stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(len(chars))))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "train model and generate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_train = np.zeros((N_Epoch,len(model.metrics_names)))\n",
    "metrics_test = np.zeros((N_Epoch,len(model.metrics_names)))\n",
    "\n",
    "print('Starting to learn:')\n",
    "for i in range(N_Epoch):\n",
    "    print('------- {} out of {} Epoch -----'.format(i+1,N_Epoch))\n",
    "\n",
    "    ## Epochs should take all data; batches presented random, reset at each end of batch_size\n",
    "    np.random.shuffle(train_idxes)\n",
    "    batches_idxes = np.reshape(train_idxes, (-1,batch_size))\n",
    "    for j, batch  in enumerate(batches_idxes):\n",
    "        print('batch {} of {}'.format(j+1,n_batch-n_test_batch))\n",
    "        for k in range(n_step):\n",
    "            metrics_train[i] += model.train_on_batch(X[batch,k*step_size:(k+1)*(step_size)], Y[batch,k*step_size:(k+1)*step_size]) #python 0:3 gives 0,1,2 (which is not intuitive at all)\n",
    "        model.reset_states() \n",
    "\n",
    "    test_batch_idxes = np.reshape(test_idxes,(-1,batch_size))\n",
    "    for test_batch in test_batch_idxes:\n",
    "        for k in range(n_step):\n",
    "            metrics_test[i] += model.test_on_batch(X[test_batch,k*step_size:(k+1)*(step_size)], Y[test_batch,k*step_size:(k+1)*step_size])\n",
    "        model.reset_states() \n",
    "\n",
    "    print('Train results:\\t {} \\n \\t {}'.format(model.metrics_names, metrics_train[i]/(n_batch-n_test_batch) ))\n",
    "    print('Test results:\\t {} \\n \\t {}'.format(model.metrics_names, metrics_test[i]/(n_test_batch) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't forget to save everything ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('embedCerg_model.h5')\n",
    "np.save('embedXdata.npy',X)\n",
    "np.save('embedydata.npy',Y)\n",
    "np.save('embedTrainMetrics', metrics_train)\n",
    "np.save('embedTestMetrics', metrics_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
